{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Character level text translator** : \n",
    "## the model will try to predict the next character from the a sequence of input characters, easier to implement has less accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "print ('imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Date preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5000\n",
      "Number of unique input tokens: 67\n",
      "Number of unique output tokens: 88\n",
      "Max sequence length for inputs: 14\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Defining the model :* (Functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training :*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 14s 3ms/step - loss: 1.1444 - acc: 0.7409 - val_loss: 1.0353 - val_acc: 0.7234\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.8384 - acc: 0.7749 - val_loss: 0.8503 - val_acc: 0.7684\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.6853 - acc: 0.8145 - val_loss: 0.7230 - val_acc: 0.7924\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.5823 - acc: 0.8336 - val_loss: 0.6547 - val_acc: 0.8105\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.5297 - acc: 0.8459 - val_loss: 0.6139 - val_acc: 0.8186\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.4919 - acc: 0.8563 - val_loss: 0.5786 - val_acc: 0.8302\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.4623 - acc: 0.8645 - val_loss: 0.5688 - val_acc: 0.8313\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.4390 - acc: 0.8705 - val_loss: 0.5307 - val_acc: 0.8448\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.4175 - acc: 0.8763 - val_loss: 0.5172 - val_acc: 0.8479\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.3977 - acc: 0.8820 - val_loss: 0.5162 - val_acc: 0.8466\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.3802 - acc: 0.8871 - val_loss: 0.4995 - val_acc: 0.8525\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.3636 - acc: 0.8919 - val_loss: 0.4883 - val_acc: 0.8568\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.3476 - acc: 0.8960 - val_loss: 0.4773 - val_acc: 0.8590\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.3333 - acc: 0.9004 - val_loss: 0.4830 - val_acc: 0.8581\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.3190 - acc: 0.9043 - val_loss: 0.4743 - val_acc: 0.8624\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.3052 - acc: 0.9085 - val_loss: 0.4721 - val_acc: 0.8635\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2932 - acc: 0.9116 - val_loss: 0.4702 - val_acc: 0.8648\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2807 - acc: 0.9153 - val_loss: 0.4687 - val_acc: 0.8665\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.2689 - acc: 0.9190 - val_loss: 0.4722 - val_acc: 0.8680\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2582 - acc: 0.9223 - val_loss: 0.4749 - val_acc: 0.8659\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2473 - acc: 0.9254 - val_loss: 0.4718 - val_acc: 0.8678\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2376 - acc: 0.9281 - val_loss: 0.4749 - val_acc: 0.8682\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2272 - acc: 0.9312 - val_loss: 0.4815 - val_acc: 0.8672\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2184 - acc: 0.9343 - val_loss: 0.4862 - val_acc: 0.8678\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2093 - acc: 0.9368 - val_loss: 0.4918 - val_acc: 0.8676\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2013 - acc: 0.9388 - val_loss: 0.4926 - val_acc: 0.8699\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1928 - acc: 0.9413 - val_loss: 0.4978 - val_acc: 0.8680\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1853 - acc: 0.9436 - val_loss: 0.5021 - val_acc: 0.8689\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1777 - acc: 0.9460 - val_loss: 0.5101 - val_acc: 0.8686\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1707 - acc: 0.9480 - val_loss: 0.5122 - val_acc: 0.8681\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1641 - acc: 0.9499 - val_loss: 0.5231 - val_acc: 0.8687\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1576 - acc: 0.9520 - val_loss: 0.5229 - val_acc: 0.8692\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1518 - acc: 0.9536 - val_loss: 0.5283 - val_acc: 0.8684\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1465 - acc: 0.9551 - val_loss: 0.5331 - val_acc: 0.8715\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1407 - acc: 0.9568 - val_loss: 0.5412 - val_acc: 0.8697\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1354 - acc: 0.9584 - val_loss: 0.5443 - val_acc: 0.8709\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1305 - acc: 0.9599 - val_loss: 0.5572 - val_acc: 0.8682\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1259 - acc: 0.9614 - val_loss: 0.5629 - val_acc: 0.8680\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1211 - acc: 0.9630 - val_loss: 0.5696 - val_acc: 0.8682\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1167 - acc: 0.9636 - val_loss: 0.5732 - val_acc: 0.8666\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1129 - acc: 0.9650 - val_loss: 0.5839 - val_acc: 0.8674\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1090 - acc: 0.9665 - val_loss: 0.5907 - val_acc: 0.8663\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1057 - acc: 0.9672 - val_loss: 0.5913 - val_acc: 0.8693\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.1018 - acc: 0.9682 - val_loss: 0.5919 - val_acc: 0.8697\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0989 - acc: 0.9693 - val_loss: 0.6096 - val_acc: 0.8689\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0955 - acc: 0.9704 - val_loss: 0.6098 - val_acc: 0.8683\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0924 - acc: 0.9711 - val_loss: 0.6207 - val_acc: 0.8677\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0894 - acc: 0.9719 - val_loss: 0.6192 - val_acc: 0.8692\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0865 - acc: 0.9727 - val_loss: 0.6298 - val_acc: 0.8669\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0841 - acc: 0.9734 - val_loss: 0.6269 - val_acc: 0.8700\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0818 - acc: 0.9743 - val_loss: 0.6480 - val_acc: 0.8666\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0790 - acc: 0.9745 - val_loss: 0.6451 - val_acc: 0.8690\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0768 - acc: 0.9756 - val_loss: 0.6569 - val_acc: 0.8669\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0739 - acc: 0.9762 - val_loss: 0.6597 - val_acc: 0.8665\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0721 - acc: 0.9769 - val_loss: 0.6588 - val_acc: 0.8694\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0701 - acc: 0.9774 - val_loss: 0.6659 - val_acc: 0.8666\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0683 - acc: 0.9777 - val_loss: 0.6736 - val_acc: 0.8678\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0665 - acc: 0.9783 - val_loss: 0.6699 - val_acc: 0.8684\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0646 - acc: 0.9790 - val_loss: 0.6795 - val_acc: 0.8678\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0627 - acc: 0.9792 - val_loss: 0.6881 - val_acc: 0.8676\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0611 - acc: 0.9798 - val_loss: 0.7005 - val_acc: 0.8677\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0590 - acc: 0.9806 - val_loss: 0.6931 - val_acc: 0.8667\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0578 - acc: 0.9807 - val_loss: 0.7014 - val_acc: 0.8664\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 9s 2ms/step - loss: 0.0564 - acc: 0.9812 - val_loss: 0.7078 - val_acc: 0.8669\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0552 - acc: 0.9813 - val_loss: 0.7031 - val_acc: 0.8670\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0539 - acc: 0.9819 - val_loss: 0.7219 - val_acc: 0.8659\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0523 - acc: 0.9823 - val_loss: 0.7182 - val_acc: 0.8672\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0508 - acc: 0.9829 - val_loss: 0.7327 - val_acc: 0.8662\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0500 - acc: 0.9830 - val_loss: 0.7263 - val_acc: 0.8654\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0488 - acc: 0.9833 - val_loss: 0.7261 - val_acc: 0.8657\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0478 - acc: 0.9836 - val_loss: 0.7302 - val_acc: 0.8668\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0469 - acc: 0.9839 - val_loss: 0.7397 - val_acc: 0.8656\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0464 - acc: 0.9840 - val_loss: 0.7349 - val_acc: 0.8672\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0450 - acc: 0.9841 - val_loss: 0.7520 - val_acc: 0.8653\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0444 - acc: 0.9844 - val_loss: 0.7538 - val_acc: 0.8651\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0432 - acc: 0.9846 - val_loss: 0.7560 - val_acc: 0.8650\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0428 - acc: 0.9849 - val_loss: 0.7545 - val_acc: 0.8661\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0417 - acc: 0.9852 - val_loss: 0.7574 - val_acc: 0.8648\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0412 - acc: 0.9854 - val_loss: 0.7668 - val_acc: 0.8656\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0407 - acc: 0.9854 - val_loss: 0.7626 - val_acc: 0.8662\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0395 - acc: 0.9858 - val_loss: 0.7648 - val_acc: 0.8657\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0395 - acc: 0.9859 - val_loss: 0.7684 - val_acc: 0.8666\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0383 - acc: 0.9863 - val_loss: 0.7813 - val_acc: 0.8658\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0385 - acc: 0.9861 - val_loss: 0.7708 - val_acc: 0.8652\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0377 - acc: 0.9863 - val_loss: 0.7748 - val_acc: 0.8661\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0372 - acc: 0.9862 - val_loss: 0.7794 - val_acc: 0.8653\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0368 - acc: 0.9862 - val_loss: 0.7848 - val_acc: 0.8665\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0362 - acc: 0.9866 - val_loss: 0.7803 - val_acc: 0.8659\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0359 - acc: 0.9867 - val_loss: 0.7775 - val_acc: 0.8647\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.0353 - acc: 0.9867 - val_loss: 0.7912 - val_acc: 0.8665\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0349 - acc: 0.9868 - val_loss: 0.7816 - val_acc: 0.8671\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0347 - acc: 0.9869 - val_loss: 0.7947 - val_acc: 0.8659\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0341 - acc: 0.9871 - val_loss: 0.7965 - val_acc: 0.8649\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0342 - acc: 0.9867 - val_loss: 0.8028 - val_acc: 0.8656\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0339 - acc: 0.9870 - val_loss: 0.8025 - val_acc: 0.8647\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0331 - acc: 0.9873 - val_loss: 0.8013 - val_acc: 0.8658\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0329 - acc: 0.9874 - val_loss: 0.8061 - val_acc: 0.8653\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0328 - acc: 0.9872 - val_loss: 0.7977 - val_acc: 0.8651\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0329 - acc: 0.9873 - val_loss: 0.7961 - val_acc: 0.8658\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0322 - acc: 0.9874 - val_loss: 0.8060 - val_acc: 0.8646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28ee95cdf28>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kino\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_5/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_5/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laoded\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('s2s.h5')\n",
    "print('laoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inference model and decode function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: I love you.\n",
      "Decoded sentence: Je t'aime !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I love you.\"\n",
    "encoded_inp = np.zeros((1, max_encoder_seq_length, num_encoder_tokens))\n",
    "encoded_inp.shape\n",
    "for t, char in enumerate(input_text):\n",
    "    encoded_inp[0, t, input_token_index[char]] = 1.\n",
    "encoded_inp[0, t + 1:, input_token_index[' ']] = 1.\n",
    "\n",
    "print('Input sentence:', input_text)\n",
    "print('Decoded sentence:', decode_sequence(encoded_inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
