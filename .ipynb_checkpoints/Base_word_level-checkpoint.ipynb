{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word level text translator** : \n",
    "## the model will try to predict the next word from the a sequence of input word, requires a big corpus and the training is longer but its results are more efficient than those of the character-level.\n",
    "## Word level translation is the most used nowadays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, CuDNNLSTM\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from unicodedata import normalize\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data preprocessing :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2124\n",
      "English Max Length: 5\n",
      "French Vocabulary Size: 4026\n",
      "French Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "#initial variables\n",
    "num_samples = 10000\n",
    "embedding_size = 256\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "#loading dataset\n",
    "with open('fra.txt','r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "#shuffle dataset before deviding it into train/test sets    \n",
    "lines = lines[: num_samples]\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "#cleaning text\n",
    "for line in lines:\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    line = line.replace('-',' ')\n",
    "    line = line.lower()\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    input_text = [word for word in input_text.split() if word.isalpha()]\n",
    "    target_text = [word for word in target_text.split() if word.isalpha()]\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "#creating and fitting tokenizers\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(input_texts)\n",
    "fr_tokenizer  = Tokenizer()\n",
    "fr_tokenizer.fit_on_texts(target_texts)\n",
    "\n",
    "input_max_len = np.max([len(line) for line in input_texts]) #longest sequence in english\n",
    "target_max_len = np.max([len(line) for line in target_texts]) #longest sequence in french\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1 #size of eng vocab, the vocab starts at 1 so we add +1 for the 0 index\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1 #same for fr\n",
    "\n",
    "#tokens dict \n",
    "fr_tokens_dict = dict((i, char) for char, i in fr_tokenizer.word_index.items())\n",
    "eng_tokens_dict = dict((i, char) for char, i in eng_tokenizer.word_index.items())\n",
    "\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (input_max_len))\n",
    "print('French Vocabulary Size: %d' % fr_vocab_size)\n",
    "print('French Max Length: %d' % (target_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize texts then fixing the length of the array with pad_sequences\n",
    "def encode_input(data, max_sequence_length):\n",
    "    x = eng_tokenizer.texts_to_sequences(data)\n",
    "    x = pad_sequences(x, maxlen=max_sequence_length, padding='post')\n",
    "    return x\n",
    "    \n",
    "#same as last function, but also transforms the data into one-hot-encoding represtation\n",
    "def encode_output(data, max_sequence_length):\n",
    "    x = fr_tokenizer.texts_to_sequences(data)\n",
    "    x = pad_sequences(x, maxlen=max_sequence_length, padding='post')\n",
    "    array = []\n",
    "    for seq in x:\n",
    "        encoded = to_categorical(seq, num_classes=fr_vocab_size)\n",
    "        array.append(encoded)\n",
    "    array = np.array(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : X =  (8000, 10) , Y =  (8000, 10, 4026)\n",
      "Test : X =  (2000, 10) , Y =  (2000, 10, 4026)\n"
     ]
    }
   ],
   "source": [
    "#splitting into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_texts, target_texts, test_size=0.20, random_state=42)\n",
    "\n",
    "#input will be max sequence length\n",
    "max_sequence_length = np.max([input_max_len, target_max_len])\n",
    "\n",
    "#vectorizing data\n",
    "X_train = encode_input(x_train, max_sequence_length)\n",
    "Y_train = encode_output(y_train, max_sequence_length)\n",
    "\n",
    "X_test = encode_input(x_test, max_sequence_length)\n",
    "Y_test = encode_output(y_test, max_sequence_length)\n",
    "\n",
    "print('Train : X = ', X_train.shape,', Y = ', Y_train.shape)\n",
    "print('Test : X = ', X_test.shape,', Y = ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining the model architecture :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 256)           543744    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (None, 256)               526336    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (None, 10, 256)           526336    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 10, 4026)          1034682   \n",
      "=================================================================\n",
      "Total params: 2,631,098\n",
      "Trainable params: 2,631,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining model\n",
    "model = Sequential()\n",
    "model.add(Embedding(eng_vocab_size, embedding_size, input_length=max_sequence_length))\n",
    "model.add(CuDNNLSTM(256))\n",
    "model.add(RepeatVector(10))\n",
    "model.add(CuDNNLSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(fr_vocab_size, activation='softmax')))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training phase :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 5s 628us/step - loss: 2.8213 - acc: 0.6681 - val_loss: 2.3084 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30838, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.2496 - acc: 0.6811 - val_loss: 2.2387 - val_acc: 0.6842\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.30838 to 2.23870, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 5s 590us/step - loss: 2.1869 - acc: 0.6850 - val_loss: 2.2185 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.23870 to 2.21850, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 5s 580us/step - loss: 2.1592 - acc: 0.6858 - val_loss: 2.2129 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.21850 to 2.21294, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 2.1405 - acc: 0.6857 - val_loss: 2.2021 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.21294 to 2.20213, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 2.1274 - acc: 0.6857 - val_loss: 2.2022 - val_acc: 0.6896\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.20213\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 2.1344 - acc: 0.6840 - val_loss: 2.2191 - val_acc: 0.6871\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.20213\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 2.1295 - acc: 0.6846 - val_loss: 2.1916 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.20213 to 2.19164, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 5s 587us/step - loss: 2.0998 - acc: 0.6856 - val_loss: 2.1865 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.19164 to 2.18651, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.0948 - acc: 0.6859 - val_loss: 2.1912 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.18651\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.0892 - acc: 0.6859 - val_loss: 2.1941 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.18651\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.0840 - acc: 0.6857 - val_loss: 2.1898 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.18651\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 5s 586us/step - loss: 2.0894 - acc: 0.6849 - val_loss: 2.1845 - val_acc: 0.6871\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.18651 to 2.18452, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.0744 - acc: 0.6859 - val_loss: 2.1828 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.18452 to 2.18275, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.0711 - acc: 0.6858 - val_loss: 2.1884 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.18275\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 5s 581us/step - loss: 2.0664 - acc: 0.6859 - val_loss: 2.1868 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.18275\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.0618 - acc: 0.6859 - val_loss: 2.1816 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.18275 to 2.18163, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 2.0367 - acc: 0.6859 - val_loss: 2.1391 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.18163 to 2.13906, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 5s 585us/step - loss: 1.9811 - acc: 0.6893 - val_loss: 2.0863 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.13906 to 2.08628, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 5s 581us/step - loss: 1.9316 - acc: 0.6943 - val_loss: 2.0584 - val_acc: 0.6999\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.08628 to 2.05840, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 5s 583us/step - loss: 1.8848 - acc: 0.6978 - val_loss: 2.0159 - val_acc: 0.7049\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.05840 to 2.01593, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 1.8103 - acc: 0.7076 - val_loss: 1.9569 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.01593 to 1.95686, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 1.7227 - acc: 0.7198 - val_loss: 1.8768 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.95686 to 1.87680, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 1.6398 - acc: 0.7273 - val_loss: 1.8270 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.87680 to 1.82702, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 1.5735 - acc: 0.7312 - val_loss: 1.7914 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.82702 to 1.79136, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 5s 583us/step - loss: 1.5051 - acc: 0.7373 - val_loss: 1.7496 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.79136 to 1.74962, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 5s 585us/step - loss: 1.4441 - acc: 0.7452 - val_loss: 1.7246 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.74962 to 1.72457, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 5s 585us/step - loss: 1.3847 - acc: 0.7521 - val_loss: 1.6892 - val_acc: 0.7503\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.72457 to 1.68917, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 5s 586us/step - loss: 1.3236 - acc: 0.7594 - val_loss: 1.6593 - val_acc: 0.7581\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.68917 to 1.65925, saving model to fra_eng_seq2seq.h5\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 5s 582us/step - loss: 1.2688 - acc: 0.7667 - val_loss: 1.6337 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.65925 to 1.63367, saving model to fra_eng_seq2seq.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19680733fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training with a checkpoint callback\n",
    "filename = 'fra_eng_seq2seq.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(X_train, Y_train, epochs=30, batch_size=64, \n",
    "          validation_data=(X_test, Y_test), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 5s 588us/step - loss: 0.3210 - acc: 0.8945 - val_loss: 1.4289 - val_acc: 0.7899\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 0.3108 - acc: 0.8966 - val_loss: 1.4354 - val_acc: 0.7920\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 0.3018 - acc: 0.8976 - val_loss: 1.4332 - val_acc: 0.7909\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 5s 588us/step - loss: 0.2918 - acc: 0.8988 - val_loss: 1.4439 - val_acc: 0.7901\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 5s 587us/step - loss: 0.2840 - acc: 0.9008 - val_loss: 1.4450 - val_acc: 0.7924\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 5s 585us/step - loss: 0.2769 - acc: 0.9010 - val_loss: 1.4390 - val_acc: 0.7909\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 0.2677 - acc: 0.9024 - val_loss: 1.4515 - val_acc: 0.7921\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 5s 586us/step - loss: 0.2604 - acc: 0.9032 - val_loss: 1.4532 - val_acc: 0.7927\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 5s 587us/step - loss: 0.2535 - acc: 0.9047 - val_loss: 1.4538 - val_acc: 0.7931\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 0.2484 - acc: 0.9046 - val_loss: 1.4613 - val_acc: 0.7924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19681439d68>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It may be possible to train for some more epochs (without a checkpoint callback ! we dont want to overwrite the last save)\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, \n",
    "          validation_data=(X_test, Y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "#saving model (Only if we have better performances)\n",
    "model.save('fra_eng_seq2seq.h5')\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "#loading model\n",
    "model.load_weights('fra_eng_seq2seq.h5')\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step\n",
      "this is crazy\n",
      "cest fou\n"
     ]
    }
   ],
   "source": [
    "x = np.expand_dims(X_test[452], axis=0)\n",
    "translation = model.predict(x, verbose=1)\n",
    "integers = [np.argmax(vector) for vector in translation[0]]\n",
    "target = list()\n",
    "for i in integers:\n",
    "    if i != 0:\n",
    "        word = fr_tokens_dict[i]\n",
    "        target.append(fr_tokens_dict[i])\n",
    "print(' '.join(x_test[452]))\n",
    "print(' '.join(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **API Model equivalent :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1013 13:29:50.970685  3920 deprecation_wrapper.py:119] From c:\\users\\kino\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1013 13:29:51.573176  3920 deprecation_wrapper.py:119] From c:\\users\\kino\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1013 13:29:51.741045  3920 deprecation_wrapper.py:119] From c:\\users\\kino\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    420700      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    212500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 512), (None, 1257472     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        [(None, None, 512),  1257472     embedding_2[0][0]                \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 4207)   2158191     cu_dnnlstm_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,306,335\n",
      "Trainable params: 5,306,335\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#API model equivalent\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "en_x = Embedding(fr_tokens_length, embedding_size)(encoder_inputs)\n",
    "encoder = CuDNNLSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dex = Embedding(eng_tokens_length, embedding_size)\n",
    "final_dex = dex(decoder_inputs)\n",
    "decoder_lstm = CuDNNLSTM(512, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)\n",
    "decoder_dense = Dense(fr_tokens_length, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
